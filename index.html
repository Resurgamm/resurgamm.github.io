---
layout: page
title: Boyi Li
subtitle: B.Eng in Computer Engineering @ ZJU - UIUC Institute
use-site-title: false
---

<head>
	<style>
		a { text-decoration : none; }
		a:hover { text-decoration : underline; }
		a, a:visited { color : #b5194f; }
	</style>
	<script src="https://kit.fontawesome.com/5bef57b3e9.js" crossorigin="anonymous"></script>
</head>

<br>
Zhonghan Zhao is a PhD student at Zhejiang University, with <a href="https://cvnext.github.io/">CVNext Lab</a> advised by Prof. <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>. His research interests include human perception model, embodied AI, multi-agent planning and multi-modality learning.

<br>
<br>
<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-regular fa-note-sticky" style="font-size:24px"></i> Selected Publications:</b>
<p><font color="grey" size="3">
Also see <a href="https://OrilinZ.github.io/publications" target="_blank">Publications Page</a> and <a href="https://scholar.google.com/citations?user=tGTa-EAAAAAJ&hl=en" target="_blank">Google Scholar</a>.
</font></p>

<ul>
	<li>
		<p style="font-size:16px"> 
			<strong>
			See and think: Embodied agent in virtual environment
			</strong>
			<br>
			<b>Zhonghan Zhao*</b>,  Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang, Gaoang Wang✉
<!-- 			<br>
			<font color="#E89B00">
			<em>International Conference on Computer Vision (ICCV), 2023</em>
			</font>
			<br> -->
			<a href="https://arxiv.org/abs/2311.15209">[Paper]</a>
			<a href="https://rese1f.github.io/STEVE/">[Code]</a>
			<br>
			<font color="grey" size="2">
			A comprehensive and visionary embodied agent in the Minecraft virtual environment comprises three key components: vision perception, language instruction, and code action.
			</font>
	  	</p>
	</li><br>
	<li>
		<p style="font-size:16px"> 
			<strong>
			UniAP: Towards Universal Animal Perception in Vision via Few-shot Learning
			</strong>
			<br>
			Meiqi Sun*, <b>Zhonghan Zhao*</b>, Wenhao Chai, Hanjun Luo, Shidong Cao, Yanting Zhang, Jenq-Neng Hwang, Gaoang Wang✉
			<br>
			<font color="#E89B00">
			<em>Association for the Advancement of Artificial Intelligence (AAAI), 2024</em>
			</font>
			<br>
			<a href="https://arxiv.org/abs/2308.09953">[Paper]</a>
			<a href="https://github.com/OrilinZ/UniAP">[Code]</a>
			<br>
			<font color="grey" size="2">
			A novel Universal Animal Perception model, UniAP that leverages few-shot learning to enable cross-species perception among various visual tasks.
			</font>
	  	</p>
	</li><br>
	<li>
		<p style="font-size:16px"> 
			<strong>
			A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision
			</strong>
			<br>
			<b>Zhonghan Zhao</b>,  Wenhao Chai, Shengyu Hao, Wenhao Hu, Guanhong Wang, Shidong Cao, Mingli Song, Jenq-Neng Hwang, Gaoang Wang✉
<!-- 			<br>
			<font color="#E89B00">
			<em>International Conference on Computer Vision (ICCV), 2023</em>
			</font>
			<br> -->
			<a href="https://arxiv.org/abs/2307.03353">[Paper]</a>
			<a href="https://github.com/OrilinZ/Awesome-DL-Sports">[Code]</a>
			<br>
			<font color="grey" size="2">
			A survey of Deep Learning in Sports Applications.
			</font>
	  	</p>
	</li>
</ul>

<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-solid fa-pen-to-square" style="font-size:24px"></i> Updates:</b><br><br>

<ul>		
	<li><i>November 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>See and think: Embodied agent in virtual environment.
	</li><be>
		
	<li><i>August 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>UniAP: Towards Universal Animal Perception in Vision via Few-shot Learning.
	</li><be>
	
	<li><i>June 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision.
	</li><br>

	<li><i>July 2022:</i> Start my research on the Active Tracking task, advised by Professor <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>.
	</li><br>

</ul>
